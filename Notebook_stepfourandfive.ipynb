{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf5CmXQCZyF1"
   },
   "source": [
    "# Guided Capstone Step 4. Pre-Processing and Training Data Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2jue2jPGJlt"
   },
   "source": [
    "**The Data Science Method**  \n",
    "\n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "\n",
    "2.   Data Wrangling \n",
    "  \n",
    " \n",
    "3.   Exploratory Data Analysis   \n",
    "\n",
    "4.   **Pre-processing and Training Data Development**  \n",
    " * Create dummy or indicator features for categorical variables\n",
    "  * Standardize the magnitude of numeric features\n",
    "  * Split into testing and training datasets\n",
    "  * Apply scaler to the testing set\n",
    "5.   Modeling \n",
    "  * Fit Models with Training Data Set\n",
    "  * Review Model Outcomes — Iterate over additional models as needed.\n",
    "  * Identify the Final Model\n",
    "\n",
    "6.   Documentation\n",
    "  * Review the Results\n",
    "  * Present and share your findings - storytelling\n",
    "  * Finalize Code \n",
    "  * Finalize Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8xfkAqqZyF2"
   },
   "source": [
    "**<font color='teal'> Start by loading the necessary packages as we did in step 3 and printing out our current working directory just to confirm we are in the correct project directory. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ry6WPL5eZyF3"
   },
   "outputs": [],
   "source": [
    "#load python packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjrek\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the path\n",
    "path=\"C:\\Springboard\\Github\\Guided Capstone\"\n",
    "os.chdir(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "630T-ogRZyF8"
   },
   "source": [
    "**<font color='teal'>  Load the csv file you created in step 3, remember it should be saved inside your data subfolder and print the first five rows.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMNbk0u3ZyF9",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name</th>\n",
       "      <th>state</th>\n",
       "      <th>summit_elev</th>\n",
       "      <th>vertical_drop</th>\n",
       "      <th>trams</th>\n",
       "      <th>fastEight</th>\n",
       "      <th>fastSixes</th>\n",
       "      <th>fastQuads</th>\n",
       "      <th>quad</th>\n",
       "      <th>...</th>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <th>Snow Making_ac</th>\n",
       "      <th>daysOpenLastYear</th>\n",
       "      <th>yearsOpen</th>\n",
       "      <th>averageSnowfall</th>\n",
       "      <th>AdultWeekday</th>\n",
       "      <th>AdultWeekend</th>\n",
       "      <th>projectedDaysOpen</th>\n",
       "      <th>NightSkiing_ac</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Hilltop Ski Area</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2090</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Sunrise Park Resort</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>11100</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>800.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Yosemite Ski &amp; Snowboard Area</td>\n",
       "      <td>California</td>\n",
       "      <td>7800</td>\n",
       "      <td>600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>84.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Boreal Mountain Resort</td>\n",
       "      <td>California</td>\n",
       "      <td>7700</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>380.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>54.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Dodge Ridge</td>\n",
       "      <td>California</td>\n",
       "      <td>8200</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>862.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.103943</td>\n",
       "      <td>69.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           Name       state  summit_elev  \\\n",
       "0           2               Hilltop Ski Area      Alaska         2090   \n",
       "1           4            Sunrise Park Resort     Arizona        11100   \n",
       "2           5  Yosemite Ski & Snowboard Area  California         7800   \n",
       "3           8         Boreal Mountain Resort  California         7700   \n",
       "4           9                    Dodge Ridge  California         8200   \n",
       "\n",
       "   vertical_drop  trams  fastEight  fastSixes  fastQuads  quad  ...  \\\n",
       "0            294      0        0.0          0          0     0  ...   \n",
       "1           1800      0        0.0          0          1     2  ...   \n",
       "2            600      0        0.0          0          0     0  ...   \n",
       "3            500      0        0.0          0          1     1  ...   \n",
       "4           1600      0        0.0          0          0     1  ...   \n",
       "\n",
       "   SkiableTerrain_ac  Snow Making_ac  daysOpenLastYear  yearsOpen  \\\n",
       "0               30.0            30.0        150.000000       36.0   \n",
       "1              800.0            80.0        115.000000       49.0   \n",
       "2               88.0             0.0        110.000000       84.0   \n",
       "3              380.0           200.0        150.000000       54.0   \n",
       "4              862.0             0.0        115.103943       69.0   \n",
       "\n",
       "   averageSnowfall  AdultWeekday  AdultWeekend  projectedDaysOpen  \\\n",
       "0             69.0          30.0          34.0              152.0   \n",
       "1            250.0          74.0          78.0              104.0   \n",
       "2            300.0          47.0          47.0              107.0   \n",
       "3            400.0          49.0          49.0              150.0   \n",
       "4            350.0          78.0          78.0              140.0   \n",
       "\n",
       "   NightSkiing_ac  clusters  \n",
       "0            30.0         2  \n",
       "1            80.0         1  \n",
       "2             0.0         1  \n",
       "3           200.0         1  \n",
       "4             0.0         1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski = pd.read_csv(\"step3_output.csv\")\n",
    "ski.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkBHf9smZyGB"
   },
   "source": [
    "## Create dummy features for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWKHm0NhAnrJ"
   },
   "source": [
    "**<font color='teal'> Create dummy variables for `state`. Add the dummies back to the dataframe and remove the original column for `state`. </font>**\n",
    "\n",
    "Hint: you can see an example of how to execute this in Aiden's article on preprocessing [here](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-pre-processing-and-training-data-development-fd2d75182967). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZqWk8ltZyGZ"
   },
   "outputs": [],
   "source": [
    "# I've not created dummy variables before, so I will break this down further\n",
    "dfo=ski['state'] # select states\n",
    "dummy = pd.get_dummies(dfo)\n",
    "ski = pd.concat([ski.drop('state', axis=1), dummy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name</th>\n",
       "      <th>summit_elev</th>\n",
       "      <th>vertical_drop</th>\n",
       "      <th>trams</th>\n",
       "      <th>fastEight</th>\n",
       "      <th>fastSixes</th>\n",
       "      <th>fastQuads</th>\n",
       "      <th>quad</th>\n",
       "      <th>triple</th>\n",
       "      <th>...</th>\n",
       "      <th>Rhode Island</th>\n",
       "      <th>South Dakota</th>\n",
       "      <th>Tennessee</th>\n",
       "      <th>Utah</th>\n",
       "      <th>Vermont</th>\n",
       "      <th>Virginia</th>\n",
       "      <th>Washington</th>\n",
       "      <th>West Virginia</th>\n",
       "      <th>Wisconsin</th>\n",
       "      <th>Wyoming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Hilltop Ski Area</td>\n",
       "      <td>2090</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Sunrise Park Resort</td>\n",
       "      <td>11100</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Yosemite Ski &amp; Snowboard Area</td>\n",
       "      <td>7800</td>\n",
       "      <td>600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Boreal Mountain Resort</td>\n",
       "      <td>7700</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Dodge Ridge</td>\n",
       "      <td>8200</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           Name  summit_elev  vertical_drop  \\\n",
       "0           2               Hilltop Ski Area         2090            294   \n",
       "1           4            Sunrise Park Resort        11100           1800   \n",
       "2           5  Yosemite Ski & Snowboard Area         7800            600   \n",
       "3           8         Boreal Mountain Resort         7700            500   \n",
       "4           9                    Dodge Ridge         8200           1600   \n",
       "\n",
       "   trams  fastEight  fastSixes  fastQuads  quad  triple  ...  Rhode Island  \\\n",
       "0      0        0.0          0          0     0       1  ...             0   \n",
       "1      0        0.0          0          1     2       3  ...             0   \n",
       "2      0        0.0          0          0     0       1  ...             0   \n",
       "3      0        0.0          0          1     1       3  ...             0   \n",
       "4      0        0.0          0          0     1       2  ...             0   \n",
       "\n",
       "   South Dakota  Tennessee  Utah  Vermont  Virginia  Washington  \\\n",
       "0             0          0     0        0         0           0   \n",
       "1             0          0     0        0         0           0   \n",
       "2             0          0     0        0         0           0   \n",
       "3             0          0     0        0         0           0   \n",
       "4             0          0     0        0         0           0   \n",
       "\n",
       "   West Virginia  Wisconsin  Wyoming  \n",
       "0              0          0        0  \n",
       "1              0          0        0  \n",
       "2              0          0        0  \n",
       "3              0          0        0  \n",
       "4              0          0        0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Unnamed: 0\" column does not belong. I'm going to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summit_elev</th>\n",
       "      <td>172.0</td>\n",
       "      <td>3637.656977</td>\n",
       "      <td>3223.022158</td>\n",
       "      <td>315.0</td>\n",
       "      <td>1245.00</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>6437.500000</td>\n",
       "      <td>12075.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vertical_drop</th>\n",
       "      <td>172.0</td>\n",
       "      <td>841.447674</td>\n",
       "      <td>584.594889</td>\n",
       "      <td>175.0</td>\n",
       "      <td>391.25</td>\n",
       "      <td>650.0</td>\n",
       "      <td>1150.000000</td>\n",
       "      <td>2610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trams</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastEight</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastSixes</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastQuads</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.238372</td>\n",
       "      <td>0.588520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quad</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.540698</td>\n",
       "      <td>0.703854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triple</th>\n",
       "      <td>172.0</td>\n",
       "      <td>1.209302</td>\n",
       "      <td>1.234175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>172.0</td>\n",
       "      <td>1.720930</td>\n",
       "      <td>1.472168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface</th>\n",
       "      <td>172.0</td>\n",
       "      <td>2.261628</td>\n",
       "      <td>1.445353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_chairs</th>\n",
       "      <td>172.0</td>\n",
       "      <td>5.970930</td>\n",
       "      <td>2.768721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runs</th>\n",
       "      <td>172.0</td>\n",
       "      <td>29.715116</td>\n",
       "      <td>17.443148</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TerrainParks</th>\n",
       "      <td>172.0</td>\n",
       "      <td>1.761628</td>\n",
       "      <td>1.412614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LongestRun_mi</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.989535</td>\n",
       "      <td>0.741585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.425000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SkiableTerrain_ac</th>\n",
       "      <td>172.0</td>\n",
       "      <td>230.034884</td>\n",
       "      <td>287.013293</td>\n",
       "      <td>8.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>117.5</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Snow Making_ac</th>\n",
       "      <td>172.0</td>\n",
       "      <td>82.005814</td>\n",
       "      <td>71.350096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysOpenLastYear</th>\n",
       "      <td>172.0</td>\n",
       "      <td>106.146641</td>\n",
       "      <td>22.026385</td>\n",
       "      <td>56.0</td>\n",
       "      <td>94.75</td>\n",
       "      <td>110.0</td>\n",
       "      <td>115.103943</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearsOpen</th>\n",
       "      <td>172.0</td>\n",
       "      <td>59.441860</td>\n",
       "      <td>14.247277</td>\n",
       "      <td>22.0</td>\n",
       "      <td>51.75</td>\n",
       "      <td>58.0</td>\n",
       "      <td>68.250000</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>averageSnowfall</th>\n",
       "      <td>172.0</td>\n",
       "      <td>145.142920</td>\n",
       "      <td>106.155149</td>\n",
       "      <td>18.0</td>\n",
       "      <td>51.75</td>\n",
       "      <td>120.0</td>\n",
       "      <td>205.500000</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdultWeekday</th>\n",
       "      <td>172.0</td>\n",
       "      <td>50.108117</td>\n",
       "      <td>16.070413</td>\n",
       "      <td>17.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>49.0</td>\n",
       "      <td>57.916957</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdultWeekend</th>\n",
       "      <td>172.0</td>\n",
       "      <td>56.397543</td>\n",
       "      <td>15.234606</td>\n",
       "      <td>20.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projectedDaysOpen</th>\n",
       "      <td>172.0</td>\n",
       "      <td>112.280652</td>\n",
       "      <td>21.200225</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>115.0</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NightSkiing_ac</th>\n",
       "      <td>172.0</td>\n",
       "      <td>44.296512</td>\n",
       "      <td>47.442787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.0</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clusters</th>\n",
       "      <td>172.0</td>\n",
       "      <td>1.430233</td>\n",
       "      <td>0.749904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.255498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.151155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Connecticut</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.029070</td>\n",
       "      <td>0.168493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idaho</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.184021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illinois</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>0.131293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.107517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iowa</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>0.131293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.029070</td>\n",
       "      <td>0.168493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maryland</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Massachusetts</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.151155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.087209</td>\n",
       "      <td>0.282965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minnesota</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.211205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.107517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>0.131293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nevada</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Hampshire</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.063953</td>\n",
       "      <td>0.245384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Jersey</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Mexico</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.184021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New York</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.321495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Carolina</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>0.131293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.151155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oregon</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>0.131293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pennsylvania</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.063953</td>\n",
       "      <td>0.245384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rhode Island</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tennessee</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utah</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>0.131293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.211205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virginia</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.107517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>0.131293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>0.131293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.234690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.029070</td>\n",
       "      <td>0.168493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count         mean          std    min      25%     50%  \\\n",
       "summit_elev        172.0  3637.656977  3223.022158  315.0  1245.00  2000.0   \n",
       "vertical_drop      172.0   841.447674   584.594889  175.0   391.25   650.0   \n",
       "trams              172.0     0.000000     0.000000    0.0     0.00     0.0   \n",
       "fastEight          172.0     0.000000     0.000000    0.0     0.00     0.0   \n",
       "fastSixes          172.0     0.000000     0.000000    0.0     0.00     0.0   \n",
       "fastQuads          172.0     0.238372     0.588520    0.0     0.00     0.0   \n",
       "quad               172.0     0.540698     0.703854    0.0     0.00     0.0   \n",
       "triple             172.0     1.209302     1.234175    0.0     0.00     1.0   \n",
       "double             172.0     1.720930     1.472168    0.0     1.00     1.0   \n",
       "surface            172.0     2.261628     1.445353    0.0     1.00     2.0   \n",
       "total_chairs       172.0     5.970930     2.768721    0.0     4.00     5.0   \n",
       "Runs               172.0    29.715116    17.443148    7.0    16.00    24.0   \n",
       "TerrainParks       172.0     1.761628     1.412614    0.0     1.00     2.0   \n",
       "LongestRun_mi      172.0     0.989535     0.741585    0.0     0.40     1.0   \n",
       "SkiableTerrain_ac  172.0   230.034884   287.013293    8.0    50.00   117.5   \n",
       "Snow Making_ac     172.0    82.005814    71.350096    0.0    30.00    60.0   \n",
       "daysOpenLastYear   172.0   106.146641    22.026385   56.0    94.75   110.0   \n",
       "yearsOpen          172.0    59.441860    14.247277   22.0    51.75    58.0   \n",
       "averageSnowfall    172.0   145.142920   106.155149   18.0    51.75   120.0   \n",
       "AdultWeekday       172.0    50.108117    16.070413   17.0    39.00    49.0   \n",
       "AdultWeekend       172.0    56.397543    15.234606   20.0    45.00    55.0   \n",
       "projectedDaysOpen  172.0   112.280652    21.200225   56.0   100.00   115.0   \n",
       "NightSkiing_ac     172.0    44.296512    47.442787    0.0     0.00    35.0   \n",
       "clusters           172.0     1.430233     0.749904    0.0     1.00     2.0   \n",
       "Alaska             172.0     0.005814     0.076249    0.0     0.00     0.0   \n",
       "Arizona            172.0     0.005814     0.076249    0.0     0.00     0.0   \n",
       "California         172.0     0.069767     0.255498    0.0     0.00     0.0   \n",
       "Colorado           172.0     0.023256     0.151155    0.0     0.00     0.0   \n",
       "Connecticut        172.0     0.029070     0.168493    0.0     0.00     0.0   \n",
       "Idaho              172.0     0.034884     0.184021    0.0     0.00     0.0   \n",
       "Illinois           172.0     0.017442     0.131293    0.0     0.00     0.0   \n",
       "Indiana            172.0     0.011628     0.107517    0.0     0.00     0.0   \n",
       "Iowa               172.0     0.017442     0.131293    0.0     0.00     0.0   \n",
       "Maine              172.0     0.029070     0.168493    0.0     0.00     0.0   \n",
       "Maryland           172.0     0.005814     0.076249    0.0     0.00     0.0   \n",
       "Massachusetts      172.0     0.023256     0.151155    0.0     0.00     0.0   \n",
       "Michigan           172.0     0.087209     0.282965    0.0     0.00     0.0   \n",
       "Minnesota          172.0     0.046512     0.211205    0.0     0.00     0.0   \n",
       "Missouri           172.0     0.011628     0.107517    0.0     0.00     0.0   \n",
       "Montana            172.0     0.017442     0.131293    0.0     0.00     0.0   \n",
       "Nevada             172.0     0.005814     0.076249    0.0     0.00     0.0   \n",
       "New Hampshire      172.0     0.063953     0.245384    0.0     0.00     0.0   \n",
       "New Jersey         172.0     0.005814     0.076249    0.0     0.00     0.0   \n",
       "New Mexico         172.0     0.034884     0.184021    0.0     0.00     0.0   \n",
       "New York           172.0     0.116279     0.321495    0.0     0.00     0.0   \n",
       "North Carolina     172.0     0.017442     0.131293    0.0     0.00     0.0   \n",
       "Ohio               172.0     0.023256     0.151155    0.0     0.00     0.0   \n",
       "Oregon             172.0     0.017442     0.131293    0.0     0.00     0.0   \n",
       "Pennsylvania       172.0     0.063953     0.245384    0.0     0.00     0.0   \n",
       "Rhode Island       172.0     0.005814     0.076249    0.0     0.00     0.0   \n",
       "South Dakota       172.0     0.005814     0.076249    0.0     0.00     0.0   \n",
       "Tennessee          172.0     0.005814     0.076249    0.0     0.00     0.0   \n",
       "Utah               172.0     0.017442     0.131293    0.0     0.00     0.0   \n",
       "Vermont            172.0     0.046512     0.211205    0.0     0.00     0.0   \n",
       "Virginia           172.0     0.011628     0.107517    0.0     0.00     0.0   \n",
       "Washington         172.0     0.017442     0.131293    0.0     0.00     0.0   \n",
       "West Virginia      172.0     0.017442     0.131293    0.0     0.00     0.0   \n",
       "Wisconsin          172.0     0.058140     0.234690    0.0     0.00     0.0   \n",
       "Wyoming            172.0     0.029070     0.168493    0.0     0.00     0.0   \n",
       "\n",
       "                           75%      max  \n",
       "summit_elev        6437.500000  12075.0  \n",
       "vertical_drop      1150.000000   2610.0  \n",
       "trams                 0.000000      0.0  \n",
       "fastEight             0.000000      0.0  \n",
       "fastSixes             0.000000      0.0  \n",
       "fastQuads             0.000000      2.0  \n",
       "quad                  1.000000      2.0  \n",
       "triple                2.000000      5.0  \n",
       "double                2.000000      6.0  \n",
       "surface               3.000000      6.0  \n",
       "total_chairs          7.000000     16.0  \n",
       "Runs                 41.000000     83.0  \n",
       "TerrainParks          2.000000      6.0  \n",
       "LongestRun_mi         1.425000      4.0  \n",
       "SkiableTerrain_ac   240.000000   1500.0  \n",
       "Snow Making_ac      126.250000    300.0  \n",
       "daysOpenLastYear    115.103943    163.0  \n",
       "yearsOpen            68.250000     95.0  \n",
       "averageSnowfall     205.500000    500.0  \n",
       "AdultWeekday         57.916957     89.0  \n",
       "AdultWeekend         67.000000     93.0  \n",
       "projectedDaysOpen   122.000000    180.0  \n",
       "NightSkiing_ac       70.500000    200.0  \n",
       "clusters              2.000000      2.0  \n",
       "Alaska                0.000000      1.0  \n",
       "Arizona               0.000000      1.0  \n",
       "California            0.000000      1.0  \n",
       "Colorado              0.000000      1.0  \n",
       "Connecticut           0.000000      1.0  \n",
       "Idaho                 0.000000      1.0  \n",
       "Illinois              0.000000      1.0  \n",
       "Indiana               0.000000      1.0  \n",
       "Iowa                  0.000000      1.0  \n",
       "Maine                 0.000000      1.0  \n",
       "Maryland              0.000000      1.0  \n",
       "Massachusetts         0.000000      1.0  \n",
       "Michigan              0.000000      1.0  \n",
       "Minnesota             0.000000      1.0  \n",
       "Missouri              0.000000      1.0  \n",
       "Montana               0.000000      1.0  \n",
       "Nevada                0.000000      1.0  \n",
       "New Hampshire         0.000000      1.0  \n",
       "New Jersey            0.000000      1.0  \n",
       "New Mexico            0.000000      1.0  \n",
       "New York              0.000000      1.0  \n",
       "North Carolina        0.000000      1.0  \n",
       "Ohio                  0.000000      1.0  \n",
       "Oregon                0.000000      1.0  \n",
       "Pennsylvania          0.000000      1.0  \n",
       "Rhode Island          0.000000      1.0  \n",
       "South Dakota          0.000000      1.0  \n",
       "Tennessee             0.000000      1.0  \n",
       "Utah                  0.000000      1.0  \n",
       "Vermont               0.000000      1.0  \n",
       "Virginia              0.000000      1.0  \n",
       "Washington            0.000000      1.0  \n",
       "West Virginia         0.000000      1.0  \n",
       "Wisconsin             0.000000      1.0  \n",
       "Wyoming               0.000000      1.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ski.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnDVhE1-ZyGF"
   },
   "source": [
    "## Standardize the magnitude of numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW3D-WlDZyGG"
   },
   "source": [
    "**<font color='teal'> Using sklearn preprocessing standardize the scale of the features of the dataframe except the name of the resort which we done't need in the dataframe for modeling, so it can be droppped here as well. Also, we want to hold out our response variable(s) so we can have their true values available for model performance review. Let's set `AdultWeekend` to the y variable as our response for scaling and modeling. Later we will go back and consider the `AdultWeekday`, `dayOpenLastYear`, and `projectedDaysOpen`. For now leave them in the development dataframe. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZL-q-KtAYI6"
   },
   "outputs": [],
   "source": [
    "# first we import the preprocessing package from the sklearn library\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df\n",
    "X = ski.drop(['Name','AdultWeekend'], axis=1)\n",
    "\n",
    "# Declare a response variable, called y, and assign it the AdultWeekend column of the df \n",
    "y = ski.AdultWeekend \n",
    "\n",
    "# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X \n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X \n",
    "X_scaled=scaler.transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAT8h4_mZyGK"
   },
   "source": [
    "## Split into training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rdS8EGeAnrW"
   },
   "source": [
    "**<font color='teal'> Using sklearn model selection import train_test_split, and create a 75/25 split with the y = `AdultWeekend`. We will start by using the adult weekend ticket price as our response variable for modeling.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSkPut0gguds"
   },
   "outputs": [],
   "source": [
    "# Import the train_test_split function from the sklearn.model_selection utility.  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y\n",
    "y = y.ravel()\n",
    "\n",
    "# Call the train_test_split() function with the first two parameters set to X_scaled and y \n",
    "# Declare four variables, X_train, X_test, y_train and y_test separated by commas \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UayqbwkWAnra"
   },
   "source": [
    "Here we start the actual modeling work. First let's fit a multiple linear regression model to predict the `AdultWeekend` price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83fkLldXFCNd"
   },
   "source": [
    "# Guided Capstone Step 5. Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbZXsVevfr9M"
   },
   "source": [
    "This is the fifth step in the Data Science Method. In the previous steps you cleaned and prepared the datasets. Now it's time to get into the most exciting part: modeling! In this exercise, you'll build three different models and compare each model's performance. In the end, you'll choose the best model for demonstrating insights to Big Mountain management.\n",
    "\n",
    "\n",
    "\n",
    "### **The Data Science Method**  \n",
    "\n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "2.   Data Wrangling \n",
    "  \n",
    "3.   Exploratory Data Analysis \n",
    " \n",
    "4.   Pre-processing and Training Data Development\n",
    "\n",
    "5.   **Modeling**\n",
    "  * Fit Models with Training Data Set\n",
    "  * Review Model Outcomes — Iterate over additional models as needed.\n",
    "  * Identify the Final Model\n",
    "\n",
    "6.   Documentation\n",
    "  * Review the Results\n",
    "  * Present and share your findings - storytelling\n",
    "  * Finalize Code \n",
    "  * Finalize Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_wfsP_-Anra"
   },
   "source": [
    "## Fit Models with a Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoI8S5SwAnrc"
   },
   "source": [
    "**<font color='teal'> Using sklearn, fit the model on your training dataset.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using these sets to create a model, I want to look at them to make sure they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.258533872598576"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.110546006066734"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43180235,  0.52934642,  0.        , ..., -0.13323468,\n",
       "        -0.248452  , -0.17303214],\n",
       "       [-0.71652628, -0.58578104,  0.        , ..., -0.13323468,\n",
       "        -0.248452  , -0.17303214],\n",
       "       [-0.60761549, -0.64582637,  0.        , ..., -0.13323468,\n",
       "        -0.248452  , -0.17303214],\n",
       "       ...,\n",
       "       [ 1.51925672,  2.02190163,  0.        , ..., -0.13323468,\n",
       "        -0.248452  , -0.17303214],\n",
       "       [ 0.85956849,  0.20338608,  0.        , ..., -0.13323468,\n",
       "        -0.248452  , -0.17303214],\n",
       "       [-0.79587557,  0.0146722 ,  0.        , ..., -0.13323468,\n",
       "        -0.248452  , -0.17303214]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.076696830622025"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is way too high and would be an outlier. What happened? I need to go back and look at my scaling again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.076696830622023"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_GFr8sRAnrd"
   },
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW6K7uOPAnre"
   },
   "outputs": [],
   "source": [
    "#all first model set\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import explained_variance_score,mean_absolute_error\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fHqz9-WAnrg"
   },
   "source": [
    "**<font color='teal'> Predict on the testing dataset and score the model performance with the y_test set and the y-pred values. The explained variance is a measure of the variation explained by the model. This is also known as the R-squared value. </font>**\n",
    "\n",
    "Hint: you will have to use the `predict()` method here as it's used in this [DSM article](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-modeling-56b4233cad1b) about modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIo01lFEAnrh"
   },
   "outputs": [],
   "source": [
    "# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4YS0WE2Anrk"
   },
   "source": [
    "## Review Model Outcomes — Iterate over additional models as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSh9sGIYAnrk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.962081174491013e+21"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You might want to use the explained_variance_score() and mean_absolute_error() metrics.\n",
    "# To do so, you will need to import them from sklearn.metrics. \n",
    "from sklearn.metrics import mean_squared_error\n",
    "# You can plug y_test and y_pred into the functions to evaluate the model\n",
    "m1_evs = explained_variance_score(y_test, y_pred)\n",
    "m1_evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihzeo8tqAnro"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168200410182.6963"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_mae = mean_absolute_error(y_test, y_pred)\n",
    "m1_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWJcOuSdAnrr"
   },
   "source": [
    "**<font color='teal'> Print the intercept value from the linear model. </font>**\n",
    "\n",
    "Hint: our linear regression model `lm` has an attribute `intercept_` for the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WzWejn6Anrt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42050102600.75715\n"
     ]
    }
   ],
   "source": [
    "print(lm.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Value is WAY off the mean Adult Weekend. There has to be some issue with this model. I will check the predicted values to see how that turned out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168200410234.68936"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted values also have a mean value that doesn't make any sense. I need to review this model to find out what's wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edajrenAAnrv"
   },
   "source": [
    "**<font color='teal'> The intercept is the mean `AdultWeekend` price for all the resorts given the other characteristics. The addition or subtraction of each of the coefficient values in the regression are numeric adjustments applied to the intercept to provide a particular observation's value for the resulting `AdultWeekend` value. Also, because we took the time to scale our x values in the training data, we can compare each of the coeeficients for the features to determine the feature importances. Print the coefficient values from the linear model and sort in descending order to identify the top ten most important features.</font>** \n",
    "\n",
    "\n",
    "Hint: make sure to review the absolute value of the coefficients, because the adjustment may be positive or negative, but what we are looking for is the magnitude of impact on our response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEKc_lmZAnrw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Coefficient\n",
      "trams              2.494771e+13\n",
      "fastEight          1.267117e+13\n",
      "fastSixes          8.457336e+12\n",
      "total_chairs       7.685073e+12\n",
      "double             4.086261e+12\n",
      "surface            4.011832e+12\n",
      "triple             3.425670e+12\n",
      "quad               1.953670e+12\n",
      "fastQuads          1.633539e+12\n",
      "New York           1.159272e+12\n",
      "Michigan           1.020338e+12\n",
      "California         9.212961e+11\n",
      "Pennsylvania       8.848261e+11\n",
      "New Hampshire      8.848261e+11\n",
      "Wisconsin          8.462645e+11\n",
      "Minnesota          7.615800e+11\n",
      "Vermont            7.615800e+11\n",
      "Idaho              6.635571e+11\n",
      "New Mexico         6.635571e+11\n",
      "Wyoming            6.075638e+11\n",
      "Connecticut        6.075638e+11\n",
      "Maine              6.075638e+11\n",
      "Ohio               5.450461e+11\n",
      "Massachusetts      5.450461e+11\n",
      "Colorado           5.450461e+11\n",
      "Illinois           4.734265e+11\n",
      "Montana            4.734265e+11\n",
      "Iowa               4.734265e+11\n",
      "Oregon             4.734265e+11\n",
      "Utah               4.734265e+11\n",
      "Washington         4.734265e+11\n",
      "West Virginia      4.734265e+11\n",
      "North Carolina     4.734265e+11\n",
      "Indiana            3.876931e+11\n",
      "Missouri           3.876931e+11\n",
      "Virginia           3.876931e+11\n",
      "Alaska             2.749455e+11\n",
      "South Dakota       2.749455e+11\n",
      "Rhode Island       2.749455e+11\n",
      "New Jersey         2.749455e+11\n",
      "Arizona            2.749455e+11\n",
      "Tennessee          2.749455e+11\n",
      "Maryland           1.335681e+07\n",
      "Nevada             1.287955e+06\n",
      "AdultWeekday       7.889648e+00\n",
      "summit_elev        3.673041e+00\n",
      "vertical_drop      2.402450e+00\n",
      "clusters           1.815674e+00\n",
      "Snow Making_ac     1.625488e+00\n",
      "averageSnowfall    1.386963e+00\n",
      "projectedDaysOpen  1.352539e+00\n",
      "LongestRun_mi      1.112793e+00\n",
      "daysOpenLastYear   1.017578e+00\n",
      "Runs               1.003418e+00\n",
      "TerrainParks       8.559570e-01\n",
      "NightSkiing_ac     7.700195e-01\n",
      "SkiableTerrain_ac  7.558594e-01\n",
      "yearsOpen          1.679688e-01\n"
     ]
    }
   ],
   "source": [
    "# You might want to make a pandas DataFrame displaying the coefficients for each state like so: \n",
    "coff_m1 = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])\n",
    "print(coff_m1.sort_values('Coefficient', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is so ridiculously off, that I can't look at these values as in a reasonable way. Instead, I'm going to redo the model using random forest to see if the result are any better before moving on to Model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7973120214187768"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(random_state=0, n_estimators=200)\n",
    "model1 = regressor.fit(X_train, y_train)\n",
    "model1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a much better model. Let me look at the same tests what was run on the other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred1 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8102400964530954"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_evs = explained_variance_score(y_test, y_pred1)\n",
    "m1_evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.170408847320527"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_mae = mean_absolute_error(y_test, y_pred1)\n",
    "m1_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look way better than what we got from the previous model. Now, let's take a look at the intercept and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Importance\n",
      "AdultWeekday         0.664255\n",
      "Snow Making_ac       0.066554\n",
      "vertical_drop        0.027955\n",
      "averageSnowfall      0.021874\n",
      "LongestRun_mi        0.021764\n",
      "Runs                 0.019431\n",
      "summit_elev          0.019369\n",
      "SkiableTerrain_ac    0.018271\n",
      "total_chairs         0.017111\n",
      "NightSkiing_ac       0.016767\n",
      "yearsOpen            0.015815\n",
      "projectedDaysOpen    0.012245\n",
      "clusters             0.011013\n",
      "North Carolina       0.008582\n",
      "triple               0.008191\n",
      "daysOpenLastYear     0.006541\n",
      "TerrainParks         0.006449\n",
      "double               0.005310\n",
      "quad                 0.005088\n",
      "Vermont              0.004478\n",
      "surface              0.004262\n",
      "Virginia             0.002577\n",
      "New Hampshire        0.002403\n",
      "New York             0.002164\n",
      "Maine                0.001560\n",
      "Michigan             0.001408\n",
      "Pennsylvania         0.001125\n",
      "Connecticut          0.001081\n",
      "Idaho                0.000925\n",
      "Wisconsin            0.000923\n",
      "Tennessee            0.000741\n",
      "West Virginia        0.000715\n",
      "fastQuads            0.000605\n",
      "Minnesota            0.000329\n",
      "Colorado             0.000310\n",
      "Ohio                 0.000236\n",
      "Oregon               0.000196\n",
      "Wyoming              0.000195\n",
      "Massachusetts        0.000187\n",
      "California           0.000185\n",
      "Utah                 0.000151\n",
      "Washington           0.000129\n",
      "New Mexico           0.000102\n",
      "Indiana              0.000097\n",
      "Rhode Island         0.000090\n",
      "South Dakota         0.000062\n",
      "Missouri             0.000049\n",
      "Iowa                 0.000048\n",
      "Illinois             0.000047\n",
      "Montana              0.000023\n",
      "Alaska               0.000006\n",
      "Arizona              0.000004\n",
      "New Jersey           0.000002\n",
      "Nevada               0.000000\n",
      "Maryland             0.000000\n",
      "fastSixes            0.000000\n",
      "fastEight            0.000000\n",
      "trams                0.000000\n"
     ]
    }
   ],
   "source": [
    "#model1.feature_importances_\n",
    "coff_m1 = pd.DataFrame(model1.feature_importances_, X.columns, columns=['Importance'])\n",
    "print(coff_m1.sort_values('Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpdALMoAAnry"
   },
   "source": [
    "**<font color='teal'>You should see that the top ten important features are different states. However, the state is not something the managers at the Big Mountain Resort can do anything about. Given that we care more about actionable traits associated with ticket pricing, rebuild the model without the state features and compare the results. </font>**\n",
    "\n",
    "Hint: Try to construct another model using exactly the steps we followed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wound up with very different results since I ran the random forest fit instead. The Adult Weekday was obviously closely related to the Adult Weekend but is a response variable that we are looking to modify as well. We should not include this in the model. I will make that adjustment and run it again for Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mHYA1BzAnrz"
   },
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pz1YXAdiAnr0"
   },
   "outputs": [],
   "source": [
    "X = ski.drop(['Name','AdultWeekend', 'AdultWeekday'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nM1EGf16Anr2"
   },
   "outputs": [],
   "source": [
    "# new values of X, so it needs to be scaled again\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled=scaler.transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y variable should remain the same, so we can use that again, but need different samples\n",
    "# Recreate the four variables: X_train, X_test, y_train and y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44404603489004113"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the next model\n",
    "regressor = RandomForestRegressor(random_state=0, n_estimators=200)\n",
    "model2 = regressor.fit(X_train, y_train)\n",
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is surprisingly a better score without considering the Adult Weekday. Let's see the other scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44979314007964954"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_evs = explained_variance_score(y_test, y_pred2)\n",
    "m2_evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.854552072800807"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_mae = mean_absolute_error(y_test, y_pred2)\n",
    "m2_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained Variance is smaller but Mean Absolute Error is larger. Either way, we can't really consider Adult Weekday ticket prices as factor for increasing the ticket price at our resort, so I would highly favor leaving this out of the model.\n",
    "Let's look at the feature importances again to see if something else should be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Importance\n",
      "Snow Making_ac       0.239645\n",
      "Runs                 0.203795\n",
      "vertical_drop        0.114670\n",
      "LongestRun_mi        0.084628\n",
      "clusters             0.036292\n",
      "summit_elev          0.035544\n",
      "total_chairs         0.032016\n",
      "yearsOpen            0.031267\n",
      "daysOpenLastYear     0.027993\n",
      "projectedDaysOpen    0.027278\n",
      "SkiableTerrain_ac    0.025470\n",
      "averageSnowfall      0.019961\n",
      "NightSkiing_ac       0.019033\n",
      "triple               0.015597\n",
      "surface              0.010897\n",
      "Vermont              0.009993\n",
      "quad                 0.009439\n",
      "double               0.008450\n",
      "fastQuads            0.007762\n",
      "North Carolina       0.007082\n",
      "TerrainParks         0.006523\n",
      "New Hampshire        0.004904\n",
      "Virginia             0.003889\n",
      "New York             0.003513\n",
      "West Virginia        0.002644\n",
      "Michigan             0.001722\n",
      "California           0.001614\n",
      "Wisconsin            0.001311\n",
      "Pennsylvania         0.001255\n",
      "Rhode Island         0.000709\n",
      "Connecticut          0.000599\n",
      "Idaho                0.000555\n",
      "Minnesota            0.000446\n",
      "Wyoming              0.000439\n",
      "Massachusetts        0.000409\n",
      "Colorado             0.000372\n",
      "Maine                0.000354\n",
      "South Dakota         0.000340\n",
      "Illinois             0.000255\n",
      "Tennessee            0.000164\n",
      "New Mexico           0.000158\n",
      "New Jersey           0.000149\n",
      "Arizona              0.000143\n",
      "Montana              0.000125\n",
      "Oregon               0.000123\n",
      "Utah                 0.000106\n",
      "Iowa                 0.000096\n",
      "Alaska               0.000074\n",
      "Washington           0.000064\n",
      "Ohio                 0.000051\n",
      "Indiana              0.000042\n",
      "Missouri             0.000039\n",
      "fastSixes            0.000000\n",
      "Maryland             0.000000\n",
      "fastEight            0.000000\n",
      "trams                0.000000\n",
      "Nevada               0.000000\n"
     ]
    }
   ],
   "source": [
    "#model2.feature_importances_\n",
    "coff_m2 = pd.DataFrame(model2.feature_importances_, X.columns, columns=['Importance'])\n",
    "print(coff_m2.sort_values('Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWjQLr3LAnr6"
   },
   "source": [
    "**<font color='teal'> When reviewing our new model coefficients, we see `summit_elev` is now in the number two spot. This is also difficult to change from a management prespective and highly correlated with `base_elev` and `vertical_drop`.  This time, rebuild the model without the state features and without the `summit_elev` and without `base_elev`and compare the results. </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I come to very different conclusion. Snowmaking looks like it takes top priority in this model. This probably is is more prominent in the smaller mountains that make up a larger part of our sample. Since there are a lot of small mountains in NY state and New England, this is very different than Northwest Montana. We should eliminate snowmaking from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXqvcn93Anr7"
   },
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eugnDNNAnr8"
   },
   "outputs": [],
   "source": [
    "X = ski.drop(['Name','AdultWeekend', 'AdultWeekday', 'Snow Making_ac'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pq0pW7G9Anr_"
   },
   "outputs": [],
   "source": [
    "# new values of X, so it needs to be scaled again\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled=scaler.transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reXlf0HAAnsG"
   },
   "outputs": [],
   "source": [
    "# y variable should remain the same, so we can use that again, but need different samples\n",
    "# Recreate the four variables: X_train, X_test, y_train and y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.430407664554544"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the next model\n",
    "regressor = RandomForestRegressor(random_state=0, n_estimators=200)\n",
    "model3 = regressor.fit(X_train, y_train)\n",
    "model3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slightly better score than the previous 2 models. Let's see how the other scores look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4377998413405766"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3_evs = explained_variance_score(y_test, y_pred3)\n",
    "m3_evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.311052325581393"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3_mae = mean_absolute_error(y_test, y_pred3)\n",
    "m3_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, the explained variance went down but the mean absolute error increased. Model 2 maybe a better selection. To be complete in this modeling, I want to run the models with the values removed that I was instructed to remove; I will do 2 more runs with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, I will eliminate the state variables as we were instructed above. I will still use the random forest approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'summit_elev', 'vertical_drop', 'trams', 'fastEight',\n",
       "       'fastSixes', 'fastQuads', 'quad', 'triple', 'double', 'surface',\n",
       "       'total_chairs', 'Runs', 'TerrainParks', 'LongestRun_mi',\n",
       "       'SkiableTerrain_ac', 'Snow Making_ac', 'daysOpenLastYear', 'yearsOpen',\n",
       "       'averageSnowfall', 'AdultWeekday', 'AdultWeekend', 'projectedDaysOpen',\n",
       "       'NightSkiing_ac', 'clusters', 'Alaska', 'Arizona', 'California',\n",
       "       'Colorado', 'Connecticut', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n",
       "       'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
       "       'Missouri', 'Montana', 'Nevada', 'New Hampshire', 'New Jersey',\n",
       "       'New Mexico', 'New York', 'North Carolina', 'Ohio', 'Oregon',\n",
       "       'Pennsylvania', 'Rhode Island', 'South Dakota', 'Tennessee', 'Utah',\n",
       "       'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin',\n",
       "       'Wyoming'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the columns to drop\n",
    "ski.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = ['Alaska', 'Arizona', 'California',\n",
    "       'Colorado', 'Connecticut', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n",
    "       'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
    "       'Missouri', 'Montana', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "       'New Mexico', 'New York', 'North Carolina', 'Ohio', 'Oregon',\n",
    "       'Pennsylvania', 'Rhode Island', 'South Dakota', 'Tennessee', 'Utah',\n",
    "       'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin',\n",
    "       'Wyoming']\n",
    "ski.drop(state_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df\n",
    "X = ski.drop(['Name','AdultWeekend'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X \n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X \n",
    "X_scaled=scaler.transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the train_test_split() function with the first two parameters set to X_scaled and y \n",
    "# Declare four variables, X_train, X_test, y_train and y_test separated by commas \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7877018421204228"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the next model\n",
    "regressor = RandomForestRegressor(random_state=0, n_estimators=200)\n",
    "model4 = regressor.fit(X_train, y_train)\n",
    "model4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a worse score; let's see how it does on the other tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8039159877302895"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_evs = explained_variance_score(y_test, y_pred4)\n",
    "m4_evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.353073053589486"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_mae = mean_absolute_error(y_test, y_pred4)\n",
    "m4_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are better than the result from model 2 or 3; this is actually a better model to exclude the states. Let's look at the importance of the factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Importance\n",
      "AdultWeekday         0.665483\n",
      "Snow Making_ac       0.068638\n",
      "vertical_drop        0.029270\n",
      "averageSnowfall      0.024280\n",
      "LongestRun_mi        0.023003\n",
      "Runs                 0.021639\n",
      "summit_elev          0.021632\n",
      "yearsOpen            0.020650\n",
      "SkiableTerrain_ac    0.020250\n",
      "total_chairs         0.017463\n",
      "NightSkiing_ac       0.016971\n",
      "clusters             0.015536\n",
      "projectedDaysOpen    0.013232\n",
      "triple               0.009156\n",
      "daysOpenLastYear     0.009039\n",
      "TerrainParks         0.007049\n",
      "double               0.005961\n",
      "quad                 0.005151\n",
      "surface              0.004826\n",
      "fastQuads            0.000771\n",
      "fastSixes            0.000000\n",
      "fastEight            0.000000\n",
      "trams                0.000000\n"
     ]
    }
   ],
   "source": [
    "#model3.feature_importances_\n",
    "coff_m4 = pd.DataFrame(model4.feature_importances_, X.columns, columns=['Importance'])\n",
    "print(coff_m4.sort_values('Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes sense that if the model includes Adult Weekday that it would be more accurate in predicting the Weekend price. This is so closely correlated that it seems like cheating, as this variable is something we are trying to determine for our own Big Mountain Resort. I don't believe this is a good approach other than to realize that the the price of weekend ticket is very closely related to the weekend ticket. This is the model with 'state' removed; now, I will follow instruction and also remove summit elevation (base elevation was already removed back in step 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vertical_drop', 'trams', 'fastEight', 'fastSixes', 'fastQuads', 'quad',\n",
       "       'triple', 'double', 'surface', 'total_chairs', 'Runs', 'TerrainParks',\n",
       "       'LongestRun_mi', 'SkiableTerrain_ac', 'Snow Making_ac',\n",
       "       'daysOpenLastYear', 'yearsOpen', 'averageSnowfall', 'AdultWeekday',\n",
       "       'projectedDaysOpen', 'NightSkiing_ac', 'clusters'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns that we don't want\n",
    "X = ski.drop(['Name','AdultWeekend', 'summit_elev'], axis=1)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look right. Now to make the split again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled=scaler.transform(X) \n",
    "# Declare four variables, X_train, X_test, y_train and y_test separated by commas \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7939868280905577"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the next model\n",
    "regressor = RandomForestRegressor(random_state=0, n_estimators=200)\n",
    "model5 = regressor.fit(X_train, y_train)\n",
    "model5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred5 = model5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8096992535606947"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_score(y_test, y_pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.297602578361983"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are slightly better than the previous model. Perhaps we drop summit elevation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJvQMns6AnsI"
   },
   "source": [
    "## Identify the Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LD7-3iLcAnsK"
   },
   "source": [
    "**<font color='teal'> Review the model performances in the table below and choose the best model for proving insights to Big Mountain management about what features are driving ski resort lift ticket prices. Type your choice in the final markdown cell — you will discuss this selection more in the next step of the guided casptone. </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "La5S9fRPAnsK"
   },
   "source": [
    "| Model | Explained Variance| Mean Absolute Error|Features Dropped|\n",
    "| --- | --- | --- | --- |\n",
    "| Model 1. | 0.810 | 6.17 |-|\n",
    "| Model 2. | 0.450 | 9.85 |'AdultWeekday'|\n",
    "| Model 3. | 0.438 | 10.31 |'AdultWeekday', 'Snow Making_ac'|\n",
    "| Model 4. | 0.804 | 6.35 |'state'|\n",
    "| Model 5. | 0.810 | 6.30 |'state', 'summit_elev'|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2c-zn7TAnsL"
   },
   "source": [
    "#### Model Selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CgC0eMBrAnsM"
   },
   "source": [
    "Model 1 and Model 5 appears to provide the best results, but we should not be including Adult Weekday price in the prediction of Adult Weekend price. Both of these are variables of interest that we will be looking to modify. The instructions stated \"Later we will go back and consider the AdultWeekday, dayOpenLastYear, and projectedDaysOpen\". I didn't see this happen in this Step of the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the context of the problem, I would select Model 5 - it has great error results that are very similar to Model 1, but without the extra variables. However, I wouldn't feel confident in any of these models to predict the lift ticket price increase for Big Mountain Resort. We had a data set that Big Mountain would have fallen inside of, letting us do an interpolation; rather than throw out some of the smaller mountains that are no similar to Big Mountain, we threw out the bigger mountains that were outliers for skiable acres and vertical. Therefore we would be extrapolating on these variables when we didn't have to. We should have thrown out the small mountains because they are not similar to what we were looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were considering adjusting the lift ticket prices for weekends and weekdays; they are closely related at ski resorts for obvious reasons. To include one in the model of of the other doesn't seem like a wise choice, as it would overpower all of the other variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RtEspslPZyGY",
    "s0DokMkAZyGc",
    "2iuitnKcZyHS",
    "iAWQxougZyHW",
    "ThMTimlBZyHZ",
    "QwZ-LkjXZyHt",
    "srtXEA3N4-Y9",
    "ChVreJupZyIA",
    "zDgSSsq1ZyID",
    "I3GYKWfi5Llg",
    "pmMvrhbI-viE",
    "ZXDPkW3UZyIX",
    "Dnc_vHQLZyId",
    "daJxuJ-dZyIg",
    "mAQ-oHiPZyIn",
    "hnGOsp3mZyIp"
   ],
   "name": "GuidedCapstoneStep5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
